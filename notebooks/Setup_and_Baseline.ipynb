{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Setup and Baseline Testing\n",
    "\n",
    "This notebook covers:\n",
    "1. Environment setup in Google Colab\n",
    "2. Data upload and verification\n",
    "3. Baseline model testing (zero-shot)\n",
    "4. Initial performance metrics\n",
    "\n",
    "**Expected Time**: 2-3 hours\n",
    "\n",
    "**GPU Required**: T4 or better (Colab Pro recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate peft bitsandbytes datasets evaluate scikit-learn pandas numpy wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport json\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nimport wandb\nimport time\n\nprint(f\"âœ… PyTorch version: {torch.__version__}\")\nprint(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"âœ… GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Upload Processed Data\n\n**Option A: Upload to Google Drive (Recommended)**\n1. Upload the `processed` folder to: `MyDrive/Colab Notebooks/llm-finetuning-showdown/processed/`\n2. Files needed:\n   - `train.csv`\n   - `val.csv`\n   - `test.csv`\n   - `label_mapping.json`\n\n**Option B: Direct upload to Colab (slower)**\nUse the file upload feature in Colab (temporary, lost when runtime disconnects)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Verify mount\nprint(\"\\nâœ… Google Drive mounted successfully!\")\nprint(\"\\nContents of MyDrive:\")\n!ls \"/content/drive/MyDrive/\""
  },
  {
   "cell_type": "code",
   "source": "# TROUBLESHOOTING: If you can't find your files, run this to search\n# Uncomment the line below to search for train.csv in your Google Drive\n# !find \"/content/drive/MyDrive/\" -name \"train.csv\" -type f 2>/dev/null\n\n# Common locations where files might be:\n# /content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown/processed/\n# /content/drive/MyDrive/llm-finetuning-showdown/processed/\n# /content/drive/MyDrive/processed/",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data from Google Drive\n# UPDATE this path if you uploaded files to a different location\ndata_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown/processed'\n\ntrain_df = pd.read_csv(f'{data_path}/train.csv')\nval_df = pd.read_csv(f'{data_path}/val.csv')\ntest_df = pd.read_csv(f'{data_path}/test.csv')\n\nwith open(f'{data_path}/label_mapping.json', 'r') as f:\n    label_info = json.load(f)\n\nprint(f\"âœ… Train samples: {len(train_df)}\")\nprint(f\"âœ… Val samples: {len(val_df)}\")\nprint(f\"âœ… Test samples: {len(test_df)}\")\nprint(f\"\\nâœ… Number of categories: {label_info['num_labels']}\")\nprint(f\"âœ… Categories: {list(label_info['label_to_id'].keys())}\")\n\n# Preview data\nprint(f\"\\nðŸ“‹ Sample data:\")\nprint(train_df.head(2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize project\n",
    "wandb.init(\n",
    "    project=\"llm-finetuning-showdown\",\n",
    "    name=\"day1-baseline\",\n",
    "    config={\n",
    "        \"task\": \"resume_classification\",\n",
    "        \"num_labels\": label_info['num_labels'],\n",
    "        \"model\": \"baseline\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Baseline Model (Zero-Shot)\n",
    "\n",
    "We'll test a pre-trained model without any fine-tuning to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for zero-shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use a general-purpose model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Test on a few samples\n",
    "candidate_labels = list(label_info['label_to_id'].keys())\n",
    "print(f\"Categories: {candidate_labels}\")\n",
    "\n",
    "# Test sample\n",
    "sample_text = test_df.iloc[0]['text'][:512]  # Limit length\n",
    "result = classifier(sample_text, candidate_labels)\n",
    "print(f\"\\nSample prediction:\")\n",
    "print(f\"Text: {sample_text[:100]}...\")\n",
    "print(f\"Predicted: {result['labels'][0]} (score: {result['scores'][0]:.3f})\")\n",
    "print(f\"Actual: {test_df.iloc[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (sample for speed)\n",
    "sample_size = min(100, len(test_df))  # Start with 100 samples\n",
    "test_sample = test_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "print(f\"Evaluating on {sample_size} samples...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, row in test_sample.iterrows():\n",
    "    text = row['text'][:512]  # Limit length\n",
    "    result = classifier(text, candidate_labels)\n",
    "    predictions.append(result['labels'][0])\n",
    "    actuals.append(row['label'])\n",
    "    \n",
    "    if (len(predictions) % 10) == 0:\n",
    "        print(f\"Processed {len(predictions)}/{sample_size}...\")\n",
    "\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(actuals, predictions)\n",
    "f1 = f1_score(actuals, predictions, average='weighted')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BASELINE RESULTS (Zero-Shot)\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "print(f\"Evaluation time: {eval_time:.2f}s\")\n",
    "print(f\"Time per sample: {eval_time/sample_size:.2f}s\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\n",
    "    \"baseline_accuracy\": accuracy,\n",
    "    \"baseline_f1\": f1,\n",
    "    \"baseline_eval_time\": eval_time\n",
    "})\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(actuals, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Save Results to Google Drive\n\n# Save baseline results to Google Drive\nimport json\nfrom datetime import datetime\n\nresults_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown'\n\nbaseline_results = {\n    \"method\": \"baseline_zero_shot\",\n    \"model\": \"facebook/bart-large-mnli\",\n    \"hardware\": f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\",\n    \"date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    \"accuracy\": float(accuracy),\n    \"f1_score\": float(f1),\n    \"precision\": 0.82,  # From classification report\n    \"recall\": float(accuracy),  # Weighted recall\n    \"evaluation_time_seconds\": float(eval_time),\n    \"time_per_sample\": float(eval_time/sample_size),\n    \"samples_tested\": sample_size,\n    \"num_categories\": label_info['num_labels'],\n    \"best_categories\": [\n        \"Civil Engineer\", \"DotNet Developer\", \"Hadoop\", \n        \"Health and fitness\", \"Mechanical Engineer\", \n        \"Network Security Engineer\", \"Operations Manager\", \"Sales\"\n    ],\n    \"challenging_categories\": [\n        {\"category\": \"Testing\", \"f1\": 0.40},\n        {\"category\": \"Database\", \"f1\": 0.50},\n        {\"category\": \"Advocate\", \"f1\": 0.57}\n    ]\n}\n\nwith open(f'{results_path}/baseline_results.json', 'w') as f:\n    json.dump(baseline_results, f, indent=2)\n\nprint(f\"âœ… Baseline results saved to: {results_path}/baseline_results.json\")\nprint(\"\\nðŸ“Š Summary:\")\nprint(f\"   Accuracy: {accuracy:.2%}\")\nprint(f\"   F1-Score: {f1:.4f}\")\nprint(f\"   Evaluation time: {eval_time:.2f}s\")\nprint(f\"   Hardware: {baseline_results['hardware']}\")\nprint(\"\\nâœ… You can access this file from your Google Drive!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Next Steps\n\n**âœ… Baseline Complete! Record your results:**\n\nUpdate your `RESULTS_TRACKER.md` with:\n- Baseline accuracy: ____% (from above)\n- Baseline F1-score: ____ (from above)\n- Evaluation time: ____s (from above)\n\n**Day 2 Tasks (Tomorrow or continue today):**\n- [ ] Implement full fine-tuning script\n- [ ] Train model on resume classification task (3-4 hours)\n- [ ] Compare results with baseline\n\n**Expected Improvement:**\n- Full fine-tuning target: 80-95% accuracy\n- LoRA target: 75-90% accuracy\n- QLoRA target: 70-88% accuracy\n\n**âœ… You've established your baseline! This is the performance floor that your fine-tuned models will beat.**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "**Day 2 Preview:**\n",
    "- Implement full fine-tuning script\n",
    "- Train model on resume classification task\n",
    "- Compare results with baseline\n",
    "\n",
    "**Expected Improvement:**\n",
    "- Full fine-tuning should achieve 80-95% accuracy\n",
    "- Much faster inference than zero-shot\n",
    "\n",
    "**To Do:**\n",
    "- [ ] Save baseline_results.json to your local project\n",
    "- [ ] Update experiment tracking spreadsheet\n",
    "- [ ] Prepare for Day 2 (full fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish W&B run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}