{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: QLoRA Fine-Tuning\n",
    "\n",
    "This notebook implements **QLoRA (Quantized LoRA)** - the most memory-efficient fine-tuning method.\n",
    "\n",
    "**Method**: QLoRA combines 4-bit quantization with LoRA for maximum efficiency\n",
    "\n",
    "**Model**: mistralai/Mistral-7B-v0.1 (4-bit quantized)\n",
    "\n",
    "**Expected Time**: 1-2 hours\n",
    "\n",
    "**GPU Required**: T4 (15GB) - uses ~4GB memory!\n",
    "\n",
    "**Target Accuracy**: 75-88% (similar to LoRA, 75% less memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (including bitsandbytes for quantization)\n",
    "!pip install -q torch transformers accelerate peft bitsandbytes datasets evaluate scikit-learn pandas numpy wandb trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nfrom datetime import datetime\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n    DataCollatorWithPadding\n)\nfrom peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\nimport wandb\n\nprint(f\"‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\ndata_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown/processed'\n\ntrain_df = pd.read_csv(f'{data_path}/train.csv')\nval_df = pd.read_csv(f'{data_path}/val.csv')\ntest_df = pd.read_csv(f'{data_path}/test.csv')\n\nwith open(f'{data_path}/label_mapping.json', 'r') as f:\n    label_info = json.load(f)\n\n# Convert text labels to numeric IDs\nlabel_to_id = label_info['label_to_id']\ntrain_df['label'] = train_df['label'].map(label_to_id)\nval_df['label'] = val_df['label'].map(label_to_id)\ntest_df['label'] = test_df['label'].map(label_to_id)\n\nprint(f\"‚úÖ Train samples: {len(train_df)}\")\nprint(f\"‚úÖ Val samples: {len(val_df)}\")\nprint(f\"‚úÖ Test samples: {len(test_df)}\")\nprint(f\"\\n‚úÖ Number of categories: {label_info['num_labels']}\")\nprint(f\"\\n‚úÖ Labels converted to numeric IDs\")\nprint(f\"   First label (should be 0-24): {train_df['label'].iloc[0]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize project\n",
    "wandb.init(\n",
    "    project=\"llm-finetuning-showdown\",\n",
    "    name=\"qlora-finetuning\",\n",
    "    config={\n",
    "        \"method\": \"qlora\",\n",
    "        \"model\": \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"quantization\": \"4-bit\",\n",
    "        \"task\": \"resume_classification\",\n",
    "        \"num_labels\": label_info['num_labels'],\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 3,\n",
    "        \"lora_r\": 8,\n",
    "        \"lora_alpha\": 16\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with 4-bit Quantization + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "print(f\"Loading model with 4-bit quantization: {model_name}\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,  # Double quantization for even more memory savings\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Compute in bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load quantized model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=label_info['num_labels'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "print(f\"‚úÖ 4-bit quantized model loaded\")\n",
    "print(f\"üíæ Memory footprint reduced by ~75%!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA (same as before)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA to quantized model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\n‚úÖ QLoRA Configuration Applied\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üéØ Parameter reduction: {100 - trainable_percent:.2f}%\")\n",
    "print(f\"üíæ Using 4-bit quantization + LoRA = Maximum efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenization function - NO PADDING HERE (let Trainer handle it)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=512\n    )\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenize\nprint(\"Tokenizing datasets...\")\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Clean up: Remove text column (no longer needed)\ntrain_dataset = train_dataset.remove_columns(['text'])\nval_dataset = val_dataset.remove_columns(['text'])\ntest_dataset = test_dataset.remove_columns(['text'])\n\n# Rename 'label' to 'labels' (required by Trainer)\ntrain_dataset = train_dataset.rename_column('label', 'labels')\nval_dataset = val_dataset.rename_column('label', 'labels')\ntest_dataset = test_dataset.rename_column('label', 'labels')\n\n# Set format for PyTorch\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nprint(f\"‚úÖ Datasets tokenized and ready\")\nprint(f\"   Final columns: {train_dataset.column_names}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Metrics and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Training arguments - Optimized for A100 80GB (QLoRA uses even less memory)\ntraining_args = TrainingArguments(\n    output_dir=\"./results_qlora\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,  # Higher LR for QLoRA\n    per_device_train_batch_size=16,  # Larger batch size for QLoRA on A100\n    per_device_eval_batch_size=32,   # Larger batch size for QLoRA on A100\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    warmup_steps=50,\n    fp16=False,  # Don't use fp16 with 4-bit quantization\n    bf16=True,   # Use bfloat16 instead\n    report_to=\"wandb\",\n    run_name=\"qlora-finetuning\"\n)\n\nprint(\"‚úÖ Training arguments configured\")\nprint(\"üöÄ Optimized for A100 80GB: batch_size=16, eval_batch_size=32\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data collator for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nprint(\"üöÄ Starting QLoRA training...\")\nprint(f\"üìä Training samples: {len(train_dataset)}\")\nprint(f\"üìä Validation samples: {len(val_dataset)}\")\nprint(f\"üíæ Using 4-bit quantization for maximum memory efficiency\")\nprint(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Track training time\nstart_time = time.time()\n\n# Train\ntrain_result = trainer.train()\n\n# Calculate training time\ntraining_time = time.time() - start_time\ntraining_hours = training_time / 3600\n\nprint(f\"\\n‚úÖ QLoRA training completed!\")\nprint(f\"‚è∞ Training time: {training_hours:.2f} hours ({training_time:.2f} seconds)\")\nprint(f\"üìà Final training loss: {train_result.training_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üß™ Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QLoRA FINE-TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f} ({test_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"Test F1-Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Test Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_results['eval_recall']:.4f}\")\n",
    "print(f\"\\nTraining Time: {training_hours:.2f} hours\")\n",
    "print(f\"Trainable Parameters: {trainable_percent:.2f}% of total\")\n",
    "print(f\"Quantization: 4-bit (75% memory reduction)\")\n",
    "print(f\"\\nBaseline Accuracy: 73.00%\")\n",
    "print(f\"Improvement over Baseline: +{(test_results['eval_accuracy']*100 - 73):.2f}%\")\n",
    "\n",
    "# Get detailed predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "id_to_label = {v: k for k, v in label_info['label_to_id'].items()}\n",
    "target_names = [id_to_label[i] for i in range(label_info['num_labels'])]\n",
    "print(classification_report(true_labels, pred_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results and QLoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Google Drive\n",
    "results_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown'\n",
    "\n",
    "qlora_results = {\n",
    "    \"method\": \"qlora\",\n",
    "    \"model\": model_name,\n",
    "    \"quantization\": \"4-bit NF4\",\n",
    "    \"date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"accuracy\": float(test_results['eval_accuracy']),\n",
    "    \"f1_score\": float(test_results['eval_f1']),\n",
    "    \"precision\": float(test_results['eval_precision']),\n",
    "    \"recall\": float(test_results['eval_recall']),\n",
    "    \"training_time_hours\": float(training_hours),\n",
    "    \"training_time_seconds\": float(training_time),\n",
    "    \"baseline_accuracy\": 0.73,\n",
    "    \"improvement_over_baseline\": float(test_results['eval_accuracy'] - 0.73),\n",
    "    \"total_parameters\": total_params,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"trainable_percent\": float(trainable_percent),\n",
    "    \"qlora_config\": {\n",
    "        \"quantization\": \"4-bit\",\n",
    "        \"quant_type\": \"nf4\",\n",
    "        \"double_quant\": True,\n",
    "        \"compute_dtype\": \"bfloat16\",\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"]\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{results_path}/qlora_results.json', 'w') as f:\n",
    "    json.dump(qlora_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_path}/qlora_results.json\")\n",
    "\n",
    "# Save QLoRA adapter\n",
    "model.save_pretrained(f'{results_path}/qlora_adapter')\n",
    "tokenizer.save_pretrained(f'{results_path}/qlora_adapter')\n",
    "print(f\"‚úÖ QLoRA adapter saved to: {results_path}/qlora_adapter\")\n",
    "print(f\"üì¶ Adapter size: ~10-50 MB (quantized base model not saved)\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\n",
    "    \"final_test_accuracy\": test_results['eval_accuracy'],\n",
    "    \"final_test_f1\": test_results['eval_f1'],\n",
    "    \"training_time_hours\": training_hours,\n",
    "    \"improvement_over_baseline\": test_results['eval_accuracy'] - 0.73,\n",
    "    \"trainable_percent\": trainable_percent,\n",
    "    \"memory_reduction\": \"75%\"\n",
    "})\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\n‚úÖ QLoRA Fine-Tuning Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Comparison\n",
    "\n",
    "**‚úÖ All Three Methods Complete!**\n",
    "\n",
    "Load your results from Google Drive and compare:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "results_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown'\n",
    "\n",
    "with open(f'{results_path}/baseline_results.json', 'r') as f:\n",
    "    baseline = json.load(f)\n",
    "\n",
    "with open(f'{results_path}/full_ft_results.json', 'r') as f:\n",
    "    full_ft = json.load(f)\n",
    "\n",
    "with open(f'{results_path}/lora_results.json', 'r') as f:\n",
    "    lora = json.load(f)\n",
    "\n",
    "with open(f'{results_path}/qlora_results.json', 'r') as f:\n",
    "    qlora = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline:     {baseline['accuracy']*100:.2f}% | 0h training\")\n",
    "print(f\"Full FT:      {full_ft['accuracy']*100:.2f}% | {full_ft['training_time_hours']:.2f}h training\")\n",
    "print(f\"LoRA:         {lora['accuracy']*100:.2f}% | {lora['training_time_hours']:.2f}h training\")\n",
    "print(f\"QLoRA:        {qlora['accuracy']*100:.2f}% | {qlora['training_time_hours']:.2f}h training\")\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "- Day 3: Create visualizations and write Medium article\n",
    "- Day 4: Write arXiv paper\n",
    "- Day 5: Publish everything!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}