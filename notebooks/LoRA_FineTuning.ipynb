{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: LoRA Fine-Tuning\n",
    "\n",
    "This notebook implements **LoRA (Low-Rank Adaptation)** - a parameter-efficient fine-tuning method.\n",
    "\n",
    "**Method**: LoRA trains only ~0.1% of parameters by adding small adapter matrices\n",
    "\n",
    "**Model**: mistralai/Mistral-7B-v0.1\n",
    "\n",
    "**Expected Time**: 1-2 hours (faster than full fine-tuning!)\n",
    "\n",
    "**GPU Required**: T4 (15GB)\n",
    "\n",
    "**Target Accuracy**: 80-90% (close to full fine-tuning performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (including PEFT for LoRA)\n",
    "!pip install -q torch transformers accelerate peft datasets evaluate scikit-learn pandas numpy wandb trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport json\nimport time\nfrom datetime import datetime\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\nimport wandb\n\nprint(f\"‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\ndata_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown/processed'\n\ntrain_df = pd.read_csv(f'{data_path}/train.csv')\nval_df = pd.read_csv(f'{data_path}/val.csv')\ntest_df = pd.read_csv(f'{data_path}/test.csv')\n\nwith open(f'{data_path}/label_mapping.json', 'r') as f:\n    label_info = json.load(f)\n\n# Convert text labels to numeric IDs\nlabel_to_id = label_info['label_to_id']\ntrain_df['label'] = train_df['label'].map(label_to_id)\nval_df['label'] = val_df['label'].map(label_to_id)\ntest_df['label'] = test_df['label'].map(label_to_id)\n\nprint(f\"‚úÖ Train samples: {len(train_df)}\")\nprint(f\"‚úÖ Val samples: {len(val_df)}\")\nprint(f\"‚úÖ Test samples: {len(test_df)}\")\nprint(f\"\\n‚úÖ Number of categories: {label_info['num_labels']}\")\nprint(f\"\\n‚úÖ Labels converted to numeric IDs\")\nprint(f\"   First label (should be 0-24): {train_df['label'].iloc[0]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize project\n",
    "wandb.init(\n",
    "    project=\"llm-finetuning-showdown\",\n",
    "    name=\"lora-finetuning\",\n",
    "    config={\n",
    "        \"method\": \"lora\",\n",
    "        \"model\": \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"task\": \"resume_classification\",\n",
    "        \"num_labels\": label_info['num_labels'],\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 3,\n",
    "        \"lora_r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=label_info['num_labels'],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"‚úÖ Base model loaded\")\n",
    "print(f\"üìä Total parameters: {base_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=16,  # LoRA scaling\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply LoRA to attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\n‚úÖ LoRA Configuration Applied\")\n",
    "print(f\"üìä Trainable parameters: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "print(f\"üìä Total parameters: {total_params:,}\")\n",
    "print(f\"üéØ Parameter reduction: {100 - trainable_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenization function - NO PADDING HERE (let Trainer handle it)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=512\n    )\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenize\nprint(\"Tokenizing datasets...\")\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Clean up: Remove text column (no longer needed)\ntrain_dataset = train_dataset.remove_columns(['text'])\nval_dataset = val_dataset.remove_columns(['text'])\ntest_dataset = test_dataset.remove_columns(['text'])\n\n# Rename 'label' to 'labels' (required by Trainer)\ntrain_dataset = train_dataset.rename_column('label', 'labels')\nval_dataset = val_dataset.rename_column('label', 'labels')\ntest_dataset = test_dataset.rename_column('label', 'labels')\n\n# Set format for PyTorch\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nprint(f\"‚úÖ Datasets tokenized and ready\")\nprint(f\"   Final columns: {train_dataset.column_names}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Metrics and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Training arguments - Optimized for A100 80GB (LoRA uses less memory)\ntraining_args = TrainingArguments(\n    output_dir=\"./results_lora\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-4,  # Higher LR than full fine-tuning\n    per_device_train_batch_size=16,  # Larger batch size for LoRA on A100\n    per_device_eval_batch_size=32,   # Larger batch size for LoRA on A100\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    warmup_steps=50,\n    fp16=False,  # Disabled for A100\n    bf16=True,   # Use BF16 - native A100 support\n    report_to=\"wandb\",\n    run_name=\"lora-finetuning\"\n)\n\nprint(\"‚úÖ Training arguments configured\")\nprint(\"üöÄ Optimized for A100 80GB: batch_size=16, eval_batch_size=32, BF16 precision\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data collator for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nprint(\"üöÄ Starting LoRA training...\")\nprint(f\"üìä Training samples: {len(train_dataset)}\")\nprint(f\"üìä Validation samples: {len(val_dataset)}\")\nprint(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Track training time\nstart_time = time.time()\n\n# Train\ntrain_result = trainer.train()\n\n# Calculate training time\ntraining_time = time.time() - start_time\ntraining_hours = training_time / 3600\n\nprint(f\"\\n‚úÖ LoRA training completed!\")\nprint(f\"‚è∞ Training time: {training_hours:.2f} hours ({training_time:.2f} seconds)\")\nprint(f\"üìà Final training loss: {train_result.training_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üß™ Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LoRA FINE-TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f} ({test_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"Test F1-Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Test Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_results['eval_recall']:.4f}\")\n",
    "print(f\"\\nTraining Time: {training_hours:.2f} hours\")\n",
    "print(f\"Trainable Parameters: {trainable_percent:.2f}% of total\")\n",
    "print(f\"\\nBaseline Accuracy: 73.00%\")\n",
    "print(f\"Improvement over Baseline: +{(test_results['eval_accuracy']*100 - 73):.2f}%\")\n",
    "\n",
    "# Get detailed predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "id_to_label = {v: k for k, v in label_info['label_to_id'].items()}\n",
    "target_names = [id_to_label[i] for i in range(label_info['num_labels'])]\n",
    "print(classification_report(true_labels, pred_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results and LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Google Drive\n",
    "results_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown'\n",
    "\n",
    "lora_results = {\n",
    "    \"method\": \"lora\",\n",
    "    \"model\": model_name,\n",
    "    \"date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"accuracy\": float(test_results['eval_accuracy']),\n",
    "    \"f1_score\": float(test_results['eval_f1']),\n",
    "    \"precision\": float(test_results['eval_precision']),\n",
    "    \"recall\": float(test_results['eval_recall']),\n",
    "    \"training_time_hours\": float(training_hours),\n",
    "    \"training_time_seconds\": float(training_time),\n",
    "    \"baseline_accuracy\": 0.73,\n",
    "    \"improvement_over_baseline\": float(test_results['eval_accuracy'] - 0.73),\n",
    "    \"total_parameters\": total_params,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"trainable_percent\": float(trainable_percent),\n",
    "    \"lora_config\": {\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"]\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{results_path}/lora_results.json', 'w') as f:\n",
    "    json.dump(lora_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_path}/lora_results.json\")\n",
    "\n",
    "# Save LoRA adapter (small file - only trained parameters)\n",
    "model.save_pretrained(f'{results_path}/lora_adapter')\n",
    "tokenizer.save_pretrained(f'{results_path}/lora_adapter')\n",
    "print(f\"‚úÖ LoRA adapter saved to: {results_path}/lora_adapter\")\n",
    "print(f\"üì¶ Adapter size: ~10-50 MB (vs 14 GB for full model)\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\n",
    "    \"final_test_accuracy\": test_results['eval_accuracy'],\n",
    "    \"final_test_f1\": test_results['eval_f1'],\n",
    "    \"training_time_hours\": training_hours,\n",
    "    \"improvement_over_baseline\": test_results['eval_accuracy'] - 0.73,\n",
    "    \"trainable_percent\": trainable_percent\n",
    "})\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\n‚úÖ LoRA Fine-Tuning Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "**‚úÖ LoRA Fine-Tuning Complete!**\n",
    "\n",
    "**Record your results:**\n",
    "- Accuracy: ____%\n",
    "- Training time: ___ hours\n",
    "- Trainable parameters: ___% of total\n",
    "- Adapter size: ~___ MB\n",
    "\n",
    "**Compare with Full Fine-Tuning:**\n",
    "- Speed improvement: ___x faster\n",
    "- Accuracy difference: ___% points\n",
    "- Memory savings: ___% less\n",
    "\n",
    "**Next:**\n",
    "- Run QLoRA fine-tuning (Day2_QLoRA_FineTuning.ipynb)\n",
    "- Compare all three methods"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}