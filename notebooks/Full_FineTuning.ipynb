{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Full Fine-Tuning\n",
    "\n",
    "This notebook implements **Full Fine-Tuning** where we update ALL parameters of the model.\n",
    "\n",
    "**Model**: meta-llama/Llama-2-7b-hf or mistralai/Mistral-7B-v0.1\n",
    "\n",
    "**Expected Time**: 3-4 hours\n",
    "\n",
    "**GPU Required**: T4 (15GB) or A100\n",
    "\n",
    "**Target Accuracy**: 85-95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate datasets evaluate scikit-learn pandas numpy wandb trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, classification_report\n",
    "import wandb\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\ndata_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown/processed'\n\ntrain_df = pd.read_csv(f'{data_path}/train.csv')\nval_df = pd.read_csv(f'{data_path}/val.csv')\ntest_df = pd.read_csv(f'{data_path}/test.csv')\n\nwith open(f'{data_path}/label_mapping.json', 'r') as f:\n    label_info = json.load(f)\n\n# Convert text labels to numeric IDs\nlabel_to_id = label_info['label_to_id']\ntrain_df['label'] = train_df['label'].map(label_to_id)\nval_df['label'] = val_df['label'].map(label_to_id)\ntest_df['label'] = test_df['label'].map(label_to_id)\n\nprint(f\"‚úÖ Train samples: {len(train_df)}\")\nprint(f\"‚úÖ Val samples: {len(val_df)}\")\nprint(f\"‚úÖ Test samples: {len(test_df)}\")\nprint(f\"\\n‚úÖ Number of categories: {label_info['num_labels']}\")\nprint(f\"‚úÖ Categories: {list(label_info['label_to_id'].keys())}\")\nprint(f\"\\n‚úÖ Labels converted to numeric IDs\")\nprint(f\"   First label (should be 0-24): {train_df['label'].iloc[0]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "# Initialize project\n",
    "wandb.init(\n",
    "    project=\"llm-finetuning-showdown\",\n",
    "    name=\"full-finetuning\",\n",
    "    config={\n",
    "        \"method\": \"full_finetuning\",\n",
    "        \"model\": \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"task\": \"resume_classification\",\n",
    "        \"num_labels\": label_info['num_labels'],\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Choose model (Mistral is faster and better than Llama-2)\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\n# Alternative: \"meta-llama/Llama-2-7b-hf\" (requires HuggingFace access token)\n\nprint(f\"Loading model: {model_name}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load model for classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=label_info['num_labels'],\n    torch_dtype=torch.bfloat16,  # Use BF16 from the start\n    device_map=\"auto\"\n)\n\n# Configure pad token id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\nprint(f\"‚úÖ Model loaded: {model_name}\")\nprint(f\"‚úÖ Total parameters: {model.num_parameters():,}\")\nprint(f\"‚úÖ Trainable parameters: {model.num_parameters():,}\")\nprint(f\"‚úÖ Gradient checkpointing: Enabled\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenization function - NO PADDING HERE (let Trainer handle it)\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        max_length=512\n    )\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenize datasets\nprint(\"Tokenizing datasets...\")\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Clean up: Remove text column (no longer needed)\ntrain_dataset = train_dataset.remove_columns(['text'])\nval_dataset = val_dataset.remove_columns(['text'])\ntest_dataset = test_dataset.remove_columns(['text'])\n\n# Rename 'label' to 'labels' (required by Trainer)\ntrain_dataset = train_dataset.rename_column('label', 'labels')\nval_dataset = val_dataset.rename_column('label', 'labels')\ntest_dataset = test_dataset.rename_column('label', 'labels')\n\n# Set format for PyTorch\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\nprint(f\"‚úÖ Datasets tokenized and ready\")\nprint(f\"   Final columns: {train_dataset.column_names}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Metrics and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\n# Training arguments - Memory optimized for A100 80GB with gradient checkpointing\ntraining_args = TrainingArguments(\n    output_dir=\"./results_full_ft\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,  # Reduced from 8 to 4 for memory\n    per_device_eval_batch_size=8,   # Reduced from 16 to 8 for memory\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    warmup_steps=100,\n    fp16=False,  # Disabled for A100\n    bf16=True,   # Use BF16 - native A100 support\n    gradient_checkpointing=True,  # Save memory by recomputing activations\n    gradient_accumulation_steps=2,  # Maintain effective batch size of 8\n    report_to=\"wandb\",\n    run_name=\"full-finetuning\"\n)\n\nprint(\"‚úÖ Training arguments configured\")\nprint(\"üöÄ Memory optimized for A100 80GB:\")\nprint(\"   - Batch size: 4 (effective: 8 with accumulation)\")\nprint(\"   - Gradient checkpointing: Enabled\")\nprint(\"   - BF16 precision\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data collator for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nprint(\"üöÄ Starting training...\")\nprint(f\"üìä Training samples: {len(train_dataset)}\")\nprint(f\"üìä Validation samples: {len(val_dataset)}\")\nprint(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Track training time\nstart_time = time.time()\n\n# Train\ntrain_result = trainer.train()\n\n# Calculate training time\ntraining_time = time.time() - start_time\ntraining_hours = training_time / 3600\n\nprint(f\"\\n‚úÖ Training completed!\")\nprint(f\"‚è∞ Training time: {training_hours:.2f} hours ({training_time:.2f} seconds)\")\nprint(f\"üìà Final training loss: {train_result.training_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"üß™ Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FULL FINE-TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f} ({test_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"Test F1-Score: {test_results['eval_f1']:.4f}\")\n",
    "print(f\"Test Precision: {test_results['eval_precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_results['eval_recall']:.4f}\")\n",
    "print(f\"\\nTraining Time: {training_hours:.2f} hours\")\n",
    "print(f\"Baseline Accuracy: 73.00%\")\n",
    "print(f\"Improvement: +{(test_results['eval_accuracy']*100 - 73):.2f}%\")\n",
    "\n",
    "# Get detailed predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "id_to_label = {v: k for k, v in label_info['label_to_id'].items()}\n",
    "target_names = [id_to_label[i] for i in range(label_info['num_labels'])]\n",
    "print(classification_report(true_labels, pred_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Google Drive\n",
    "results_path = '/content/drive/MyDrive/Colab Notebooks/llm-finetuning-showdown'\n",
    "\n",
    "full_ft_results = {\n",
    "    \"method\": \"full_finetuning\",\n",
    "    \"model\": model_name,\n",
    "    \"date\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"accuracy\": float(test_results['eval_accuracy']),\n",
    "    \"f1_score\": float(test_results['eval_f1']),\n",
    "    \"precision\": float(test_results['eval_precision']),\n",
    "    \"recall\": float(test_results['eval_recall']),\n",
    "    \"training_time_hours\": float(training_hours),\n",
    "    \"training_time_seconds\": float(training_time),\n",
    "    \"baseline_accuracy\": 0.73,\n",
    "    \"improvement_over_baseline\": float(test_results['eval_accuracy'] - 0.73),\n",
    "    \"total_parameters\": model.num_parameters(),\n",
    "    \"trainable_parameters\": model.num_parameters(),\n",
    "    \"config\": {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 4,\n",
    "        \"epochs\": 3,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{results_path}/full_ft_results.json', 'w') as f:\n",
    "    json.dump(full_ft_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_path}/full_ft_results.json\")\n",
    "\n",
    "# Save model (optional - large file)\n",
    "# model.save_pretrained(f'{results_path}/full_ft_model')\n",
    "# tokenizer.save_pretrained(f'{results_path}/full_ft_model')\n",
    "# print(f\"‚úÖ Model saved to: {results_path}/full_ft_model\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\n",
    "    \"final_test_accuracy\": test_results['eval_accuracy'],\n",
    "    \"final_test_f1\": test_results['eval_f1'],\n",
    "    \"training_time_hours\": training_hours,\n",
    "    \"improvement_over_baseline\": test_results['eval_accuracy'] - 0.73\n",
    "})\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\n‚úÖ Full Fine-Tuning Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "**‚úÖ Full Fine-Tuning Complete!**\n",
    "\n",
    "**Record your results:**\n",
    "- Accuracy: ____%\n",
    "- Training time: ___ hours\n",
    "- GPU memory used: ___ GB (check nvidia-smi)\n",
    "\n",
    "**Next:**\n",
    "- Run LoRA fine-tuning (Day2_LoRA_FineTuning.ipynb)\n",
    "- Compare efficiency and accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}